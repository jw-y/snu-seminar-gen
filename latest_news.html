<p>Last update: 2025-07-30 08:25:10</p>
<div style="white-space: pre-wrap; font-family: monospace; background: #f9f9f9; padding: 1em; border: 1px solid #ccc;">
Seoul National University

Data Science BK21xERC Seminar

a-* ae
7 ~ __ MBtH8tal 43-2E'B1025 -
a 4

“Theory of the LoRA fine-tuning landscape

Invited Speaker
Prof. Emest Ryu

Assistant Professor

Department of Mathematics
UCLA

‘Seoul National University

& Zz BASRA MECEPHEIERO)
Graduate School of Data Science BK 21com  Geiap 27HAL2S SUE ater te]

@

</div><div class="container">
<div class="row">
<div class="col-lg-12">
<div class="single-title">
<h2>
                            [BK21xERC 세미나] UCLA Ernest Ryu 교수,  Theory of the LoRA fine-tuning landscape, 7월 7일 (월)                        </h2>
<h4>
                            July 2, 2025                        </h4>
</div>
</div>
</div>
<div class="row">
<div class="col-lg-12">
<div class="page-container-content">
<div class="wp-block-image">
<figure class="aligncenter size-full"><img alt="" class="wp-image-16352" decoding="async" height="841" sizes="(max-width: 634px) 100vw, 634px" src="https://gsds.snu.ac.kr/wp-content/uploads/2025/07/세미나.png" srcset="https://gsds.snu.ac.kr/wp-content/uploads/2025/07/세미나.png 634w, https://gsds.snu.ac.kr/wp-content/uploads/2025/07/세미나-226x300.png 226w" width="634"/></figure></div>
<hr class="wp-block-separator has-alpha-channel-opacity">
<p>일시: 2025년 7월 7일 월요일 오후 2시<br/>장소: 서울대학교 43-2동 B102호<br/>연사: Prof. Ernest Ryu, UCLA</p>
<hr class="wp-block-separator has-alpha-channel-opacity"/>
<p><strong>Title: Theory of the LoRA fine-tuning landscape</strong></p>
<p><strong>Abstract:</strong><br/>Low-rank adaptation (LoRA) has emerged as a standard approach for fine-tuning large foundation models due to its strong empirical performance, but the theoretical foundations of LoRA remain underexplored. In this talk, we present two analyses that demystify why LoRA often succeeds. In the first analysis, we adopt the Neural Tangent Kernel (NTK) assumption, which posits that the first-order Taylor expansion remains accurate throughout fine-tuning. We show that when fine-tuning with $N$ data points, there exists a low-rank solution of rank $r\lesssim \sqrt{N}$ that can be found using stochastic gradient descent (SGD), as the loss landscape has no spurious local minima. In the second analysis, we assume a priori that a low-rank update exists but do not rely on any linearization. We show that LoRA training converges to a global minimizer with low rank and small magnitude because spurious local minima have high rank and large magnitude, and the zero-initialization and weight decay induce an implicit bias toward the low-rank, small-magnitude region of the parameter space, where the global minima reside.</p>
<p><strong>This talk is based on the following two papers:</strong><br/>– U. Jang, J. D. Lee, and E. K. Ryu, LoRA training in the NTK regime has no spurious local minima, International Conference on Machine Learning (Oral, top 144/9473=1.5% of papers), 2024.<br/>– J. Kim, J. Kim, and E. K. Ryu, LoRA training provably converges to a low-rank global minimum or it fails loudly (but it probably won’t fail), International Conference on Machine Learning (Oral, top 120/12107=1.0% of papers), 2025.</p>
<p><strong>Bio:</strong><br/>Ernest Ryu is an assistant professor in the Department of Mathematics at UCLA. His current research focus is on applied mathematics, deep learning, and optimization.<br/>Professor Ryu received a B.S. degree in Physics and Electrical Engineering with honors at the California Institute of Technology in 2010 and an M.S. in Statistics and a Ph.D. in Computational and Mathematical Engineering with the Gene Golub Best Thesis Award at Stanford University in 2016. In 2016, he joined the Department of Mathematics at UCLA as an Assistant Adjunct Professor. In 2020, he joined the Department of Mathematical Sciences at Seoul National University as a tenure-track faculty member. In 2024, returned to UCLA as an assistant professor.</p>
</hr></div>
</div>
</div>
<div class="row">
<div class="col-lg-12">
<div class="attachment">
</div>
</div>
</div>
</div>